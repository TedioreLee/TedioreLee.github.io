<html lang="en">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>🌈大数据平台全生态搭建 - 𝒯𝐸𝒟𝐼𝒪𝑅𝐸𝐿𝐸𝐸</title>
<link rel="shortcut icon" href="https://leejieun.fan/favicon.ico">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css">
<link rel="stylesheet" href="https://leejieun.fan/media/css/tailwind.css">
<link rel="stylesheet" href="https://leejieun.fan/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="🌈大数据平台全生态搭建 - 𝒯𝐸𝒟𝐼𝒪𝑅𝐸𝐿𝐸𝐸 - Atom Feed" href="https://leejieun.fan/atom.xml">

    

  <meta name="description" content="
For Graduation Papers


安装虚拟机
VMWARE安装ubuntu-server16.04，步骤略过
修改Ubuntu的root用户名
输入sudo passwd root，键入你的本地用户密码，随后更新你的root..." />
  <meta property="og:title" content="🌈大数据平台全生态搭建 - 𝒯𝐸𝒟𝐼𝒪𝑅𝐸𝐿𝐸𝐸">
  <meta property="og:description" content="
For Graduation Papers


安装虚拟机
VMWARE安装ubuntu-server16.04，步骤略过
修改Ubuntu的root用户名
输入sudo passwd root，键入你的本地用户密码，随后更新你的root..." />
  <meta property="og:type" content="articles">
  <meta property="og:url" content="https://leejieun.fan/post/big-data-platform/" />
  <meta property="og:image" content="https://tediorelee-1257666331.cos.ap-chengdu.myqcloud.com/Gridea/20190517135603.png">
  <meta property="og:image:height" content="630">
  <meta property="og:image:width" content="1200">
  <meta name="twitter:title" content="🌈大数据平台全生态搭建 - 𝒯𝐸𝒟𝐼𝒪𝑅𝐸𝐿𝐸𝐸">
  <meta name="twitter:description" content="
For Graduation Papers


安装虚拟机
VMWARE安装ubuntu-server16.04，步骤略过
修改Ubuntu的root用户名
输入sudo passwd root，键入你的本地用户密码，随后更新你的root...">
  <meta name="twitter:card" content="summary_large_image">
  <link rel="canonical" href="https://leejieun.fan/post/big-data-platform/">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
 
  
    <link rel="stylesheet" href="https://leejieun.fan/media/css/prism-synthwave84.css">
  

  
</head>

<body>
  <div class="antialiased flex flex-col min-h-screen" id="app">
    <a href="https://leejieun.fan" class="fixed top-0 left-0 mt-4 bg-black text-white dark:text-gray-700 dark:bg-yellow-50 dark:hover:bg-black dark:hover:text-white inline-flex p-2 pl-8 hover:text-gray-700 hover:bg-yellow-50 font-bold z-10 transition-fast animated fadeInLeft">
      𝒯𝐸𝒟𝐼𝒪𝑅𝐸𝐿𝐸𝐸
    </a>
    <div class="max-w-4xl w-full mx-auto">
      <div class="shadow-box bg-white dark:bg-gray-600 rounded-lg pt-32 md:pt-64 px-4 md:px-8 pb-8 animated fadeIn mb-8">
        <h1 class="text-5xl font-semibold leading-normal pb-8 mb-8 border-b-8 border-gray-700">
          🌈大数据平台全生态搭建
        </h1>
        
          <img src="https://tediorelee-1257666331.cos.ap-chengdu.myqcloud.com/Gridea/20190517135603.png" alt="🌈大数据平台全生态搭建" class="block w-full mb-8">
        
        <div class="mb-8 flex flex-wrap">
          <div class="text-gray-400 text-sm mr-4">2019-04-15 · 25 min read</div>
          
            <a href="https://leejieun.fan/tag/sYpHDKKCm/" class="text-gray-700 text-sm border-b-2 border-dotted border-gray-200 hover:border-gray-600 transition-all duration-100 inline-flex mr-2">
              <i class="ri-hashtag"></i>
              Bigdata
            </a>
          
        </div>
        <div class="markdown mb-8" v-pre>
          <blockquote>
<p>For Graduation Papers</p>
</blockquote>
<!--more-->
<h2 id="安装虚拟机">安装虚拟机</h2>
<p>VMWARE安装ubuntu-server16.04，步骤略过</p>
<h3 id="修改ubuntu的root用户名">修改Ubuntu的root用户名</h3>
<p>输入<code>sudo passwd root</code>，键入你的本地用户密码，随后更新你的root用户密码</p>
<figure data-type="image" tabindex="1"><img src="https://i.loli.net/2019/04/09/5cac09d65105c.png" alt="" loading="lazy"></figure>
<p>使用<code>su root</code>即可切换到root用户下</p>
<h3 id="配置静态ip">配置静态IP</h3>
<p>使用root身份编辑<code>/etc/network/interface</code>下的<code>eth0</code>网卡</p>
<figure data-type="image" tabindex="2"><img src="https://i.loli.net/2019/04/09/5cac09fa5c64a.png" alt="" loading="lazy"></figure>
<p>以我本地网关为<code>192.168.6.1</code>为例，将虚拟机IP定为<code>192.168.6.22</code>，DNS填上本地电信的DNS</p>
<figure data-type="image" tabindex="3"><img src="https://i.loli.net/2019/04/09/5cac0a317d82e.png" alt="" loading="lazy"></figure>
<p>保存退出，使用命令<code>ifdown eth0</code>卸载网卡，再使用命令<code>ifup eth0</code>重新启用网卡即可使修改生效。</p>
<figure data-type="image" tabindex="4"><img src="https://i.loli.net/2019/04/09/5cac0a50a0bda.png" alt="" loading="lazy"></figure>
<p>这时候再用<code>ifconfig</code>查看网卡配置即可看到修改成功。</p>
<figure data-type="image" tabindex="5"><img src="https://i.loli.net/2019/04/09/5cac0a6827f6d.png" alt="" loading="lazy"></figure>
<h3 id="配置ssh登录">配置SSH登录</h3>
<p>root用户下，使用命令<code>apt-get install ssh</code></p>
<p>等待安装完成即可</p>
<p>vim编辑<code>etc/ssh/sshd_config</code></p>
<p>找到<code>PermitRootLogin</code>字段,修改为<code>yes</code></p>
<p>保存退出</p>
<figure data-type="image" tabindex="6"><img src="https://i.loli.net/2019/04/09/5cac0a84c7cb6.png" alt="" loading="lazy"></figure>
<p><code>service ssh restart</code>重启ssh服务</p>
<figure data-type="image" tabindex="7"><img src="https://i.loli.net/2019/04/09/5cac0adf0c397.png" alt="" loading="lazy"></figure>
<p>使用ssh工具即可连接</p>
<figure data-type="image" tabindex="8"><img src="https://i.loli.net/2019/04/09/5cac0af8412f6.png" alt="" loading="lazy"></figure>
<h3 id="修改软件源">修改软件源</h3>
<p>找到<a href="https://opsx.alibaba.com/mirror">阿里巴巴开源镜像站</a>里面的Ubuntu，点击后面的帮助</p>
<figure data-type="image" tabindex="9"><img src="https://i.loli.net/2019/04/09/5cac0b1822858.png" alt="" loading="lazy"></figure>
<p>选择你对应的Ubuntu版本，复制软件源信息，粘贴到<code>/etc/apt/sources.list</code>里</p>
<figure data-type="image" tabindex="10"><img src="https://i.loli.net/2019/04/09/5cac0b334dfb8.png" alt="" loading="lazy"></figure>
<p>保存退出即可，之后我们可以<code>apt-get update</code>一下，重新更新一下软件源</p>
<figure data-type="image" tabindex="11"><img src="https://i.loli.net/2019/04/09/5cac0b4e08e03.png" alt="" loading="lazy"></figure>
<h2 id="安装jdk">安装JDK</h2>
<p>新建<code>mkdir /usr/local/jdk</code>文件夹</p>
<p>使用Winscp工具上传jdk包到<code>/usr/local</code>文件夹里</p>
<p><code>tar -zxvf jdkxxx.tar.gz</code>解压缩</p>
<figure data-type="image" tabindex="12"><img src="https://i.loli.net/2019/04/09/5cac0b81b3c31.png" alt="" loading="lazy"></figure>
<p>添加jdk的环境变量</p>
<p>打开文件<code>vim /etc/profile</code>在末尾加上</p>
<figure data-type="image" tabindex="13"><img src="https://i.loli.net/2019/04/09/5cac0b9bda116.png" alt="" loading="lazy"></figure>
<pre><code>export JAVA_HOME=/usr/local/jdk/jdk1.8.0_201
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.ja$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin
</code></pre>
<p>保存并退出，<code>source /etc/profile</code>使其生效<br>
这时输入<code>java -version</code>即可查看到版本信息表示jdk安装成功</p>
<figure data-type="image" tabindex="14"><img src="https://i.loli.net/2019/04/09/5cac0bb5f34c7.png" alt="" loading="lazy"></figure>
<h2 id="配置ssh免密登录">配置SSH免密登录</h2>
<p>这里的主机名我定义为<strong>master</strong></p>
<p>修改<code>/etc/hosts</code>，把<strong>master</strong>的ip地址修改为<code>127.0.0.1</code>，保存退出</p>
<figure data-type="image" tabindex="15"><img src="https://i.loli.net/2019/04/09/5cac0bfa8d0b9.png" alt="" loading="lazy"></figure>
<p>进入<code>.ssh</code>文件夹，删除原有多余的公钥和私钥</p>
<p><code>ssh-keygen</code>生成新的ssh密钥，一路回车确定</p>
<figure data-type="image" tabindex="16"><img src="https://i.loli.net/2019/04/09/5cac0c19d61be.png" alt="" loading="lazy"></figure>
<p>将公钥复制到<strong>authorized_keys</strong>里面</p>
<pre><code>cat id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
</code></pre>
<figure data-type="image" tabindex="17"><img src="https://i.loli.net/2019/04/09/5cac0c30f1d03.png" alt="" loading="lazy"></figure>
<p>测试登录，<code>ssh master</code></p>
<figure data-type="image" tabindex="18"><img src="https://i.loli.net/2019/04/09/5cac0c4669e8c.png" alt="" loading="lazy"></figure>
<p>成功实现ssh免密登录</p>
<h2 id="安装hadoop">安装Hadoop</h2>
<h3 id="介绍">介绍</h3>
<blockquote>
<p>Hadoop是一个由Apache基金会所开发的分布式系统基础架构。Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。</p>
<p>HDFS，Hadoop Distributed File System，是一个分布式文件系统，用来存储 Hadoop 集群中所有存储节点上的文件，包含一个 NameNode 和大量 DataNode。NameNode，它在 HDFS 内部提供元数据服务，负责管理文件系统名称空间和控制外部客户机的访问，决定是否将文件映射到 DataNode 上。DataNode，它为 HDFS 提供存储块，响应来自 HDFS 客户机的读写请求。</p>
<p>MapReduce是一种编程模型，用于大规模数据集的并行运算。概念&quot;Map（映射）“和&quot;Reduce（归约）”，是它们的主要思想，即指定一个Map（映射）函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce（归约）函数，用来保证所有映射的键值对中的每一个共享相同的键组。</p>
</blockquote>
<p>在<code>/usr/local</code>中新建<code>hadoop</code>文件夹，使用winscp上传hadoop安装包</p>
<p>解压缩hadoop<code>tar -zxvf hadoop-2.6.4.tar.gz</code></p>
<figure data-type="image" tabindex="19"><img src="https://i.loli.net/2019/04/09/5cac0c7feff1e.png" alt="" loading="lazy"></figure>
<h3 id="配置环境变量">配置环境变量</h3>
<p><code>vim /etc/profile</code>，后面加上</p>
<pre><code>export HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.7
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export
CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
</code></pre>
<figure data-type="image" tabindex="20"><img src="https://i.loli.net/2019/04/09/5cac0c9793d5c.png" alt="" loading="lazy"></figure>
<p><code>source /etc/profile</code>使其生效</p>
<p>编辑<code>hadoop-env.sh、mapred-env.sh、yarn-env.sh</code>文件，修改<code>JAVA_HOME</code>参数为</p>
<pre><code>export JAVA_HOME=&quot;/usr/local/jdk&quot;
</code></pre>
<figure data-type="image" tabindex="21"><img src="https://i.loli.net/2019/04/09/5cac0cae3f607.png" alt="" loading="lazy"></figure>
<h3 id="配置core-sitexml">配置core-site.xml</h3>
<p><code>vim {HADOOP_HOME}/etc/hadoop/core-site.xml</code></p>
<p>修改为：</p>
<pre><code>&lt;configuration&gt;
	&lt;property&gt;
&lt;name&gt;fs.defaultFS&lt;/name&gt;
&lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
   &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
    &lt;value&gt;/usr/local/data&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<figure data-type="image" tabindex="22"><img src="https://i.loli.net/2019/04/09/5cac0cc6079e4.png" alt="" loading="lazy"></figure>
<p><strong>注意</strong>：这里的<code>hadoop.tmp.dir</code>修改为自己创建的临时文件存放目录</p>
<h3 id="配置hdfs-sitexml">配置hdfs-site.xml</h3>
<p><code>vim ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml</code></p>
<p>修改为：</p>
<pre><code>&lt;configuration&gt;
&lt;property&gt;
  &lt;name&gt;dfs.name.dir&lt;/name&gt;
  &lt;value&gt;/usr/local/data/namenode&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;dfs.data.dir&lt;/name&gt;
  &lt;value&gt;/usr/local/data/datanode&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;dfs.replication&lt;/name&gt;
  &lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<figure data-type="image" tabindex="23"><img src="https://i.loli.net/2019/04/09/5cac0d1732149.png" alt="" loading="lazy"></figure>
<p><strong>注意</strong>：因为这里是伪分布式环境只有一个节点，所以这里设置为1</p>
<h3 id="配置mapred-sitexml">配置mapred-site.xml</h3>
<p>修改文件名</p>
<p><code>cp mapred-site.xml.template mapred-site.xml</code></p>
<p><code>vim ${HADOOP_HOME}/etc/hadoop/mapred-site.xml</code></p>
<pre><code>&lt;configuration&gt;
&lt;property&gt;
  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
  &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<figure data-type="image" tabindex="24"><img src="https://i.loli.net/2019/04/09/5cac0d8a7317a.png" alt="" loading="lazy"></figure>
<h3 id="配置yarn-sitexml">配置yarn-site.xml</h3>
<p><code>vim ${HADOOP_HOME}/etc/hadoop/yarn-site.xml</code></p>
<pre><code>&lt;configuration&gt;
&lt;!-- Site specific YARN configuration properties --&gt;
&lt;property&gt;
  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
  &lt;value&gt;master&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;              
  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;     
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<figure data-type="image" tabindex="25"><img src="https://i.loli.net/2019/04/09/5cac136863441.png" alt="" loading="lazy"></figure>
<h3 id="测试">测试</h3>
<ul>
<li>格式化hdfs</li>
</ul>
<p><code>hdfs namenode –format</code></p>
<figure data-type="image" tabindex="26"><img src="https://i.loli.net/2019/04/09/5cac137f2dba0.png" alt="" loading="lazy"></figure>
<ul>
<li>启动hdfs</li>
</ul>
<pre><code>root@bigdata-platform:/usr/local# cd /usr/local/hadoop/hadoop-2.7.7/
root@bigdata-platform:/usr/local/hadoop/hadoop-2.7.7# sbin/start-dfs.sh

</code></pre>
<figure data-type="image" tabindex="27"><img src="https://i.loli.net/2019/04/09/5cac158d47114.png" alt="" loading="lazy"></figure>
<ul>
<li>启动yarn</li>
</ul>
<pre><code>root@bigdata-platform:/usr/local/hadoop/hadoop-2.7.7# sbin/start-yarn.sh

</code></pre>
<figure data-type="image" tabindex="28"><img src="https://i.loli.net/2019/04/09/5cac15b8ab4c8.png" alt="" loading="lazy"></figure>
<ul>
<li>jps查看</li>
</ul>
<figure data-type="image" tabindex="29"><img src="https://i.loli.net/2019/04/09/5cac15cbbd1d1.png" alt="" loading="lazy"></figure>
<p>可以看到所有节点都启动成功，打开<code>http://192.168.6.22:50070</code>可以查看可视化页面。</p>
<figure data-type="image" tabindex="30"><img src="https://i.loli.net/2019/04/09/5cac15f1f0b6a.png" alt="" loading="lazy"></figure>
<p>到此为止，HADOOP的安装就完成了。</p>
<h2 id="安装hive">安装Hive</h2>
<h3 id="介绍-2">介绍</h3>
<blockquote>
<p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过和SQL类似的HiveQL语言快速实现简单的MapReduce统计,不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。</p>
<p>Hive 没有专门的数据格式。所有Hive 的数据都存储在Hadoop兼容的文件系统（例如HDFS）中。Hive 在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS中Hive 设定的目录下，因此，Hive 不支持对数据的改写和添加，所有的数据都是在加载的时候确定的。</p>
</blockquote>
<h3 id="环境配置">环境配置</h3>
<p><strong>注意</strong>：Hive只需要在master节点上安装配置</p>
<p>在<code>usr/local</code>下新建hive文件夹，使用winscp上传hive安装包</p>
<p><code>cd /usr/local/hive</code>进入hive文件夹，使用<code>tar -zxvf apache-hive-2.3.4-bin.tar.gz</code>解压</p>
<figure data-type="image" tabindex="31"><img src="https://i.loli.net/2019/04/09/5cac1627b07ab.png" alt="" loading="lazy"></figure>
<p><strong>添加Hive环境变量</strong></p>
<p>在<code>etc/profile</code>中，添加</p>
<pre><code>export JAVA_HOME=/usr/local/java/jdk1.8.0_191
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.7
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HIVE_HOME=/usr/local/hive/hive-2.3.4
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin

</code></pre>
<p>和原有的变量整合一下，如图</p>
<figure data-type="image" tabindex="32"><img src="https://i.loli.net/2019/04/09/5cac163e4b5d2.png" alt="" loading="lazy"></figure>
<p><code>source /etc/profile</code>保存一下即可</p>
<h3 id="配置hive-sitexml">配置hive-site.xml</h3>
<p>进入hive文件夹，复制一份<strong>hive-site.xml</strong></p>
<pre><code>cp hive-default.xml.template   hive-site.xml

</code></pre>
<p>由于<strong>hive-site.xml</strong>文件过长，最好复制到虚拟机外修改之后再拷贝回来</p>
<pre><code> &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
    &lt;value&gt;jdbc:mysql://master:3306/hive_metadata?createDatabaseIfNotExist=true&lt;/value&gt;
    &lt;description&gt;
      JDBC connect string for a JDBC metastore.
      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
      For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
    &lt;/description&gt;
 &lt;/property&gt;
 
 &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
    &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
 &lt;/property&gt;
  
 &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
    &lt;value&gt;hive&lt;/value&gt;
    &lt;description&gt;Username to use against metastore database&lt;/description&gt;
 &lt;/property&gt;
    
 &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
    &lt;value&gt;hive&lt;/value&gt;
    &lt;description&gt;password to use against metastore database&lt;/description&gt;
 &lt;/property&gt;

 &lt;property&gt;
    &lt;name&gt;hive.querylog.location&lt;/name&gt;
    &lt;value&gt;/usr/local/hive/hive-2.3.4/tmp/hadoop&lt;/value&gt;
    &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;
  &lt;/property&gt;
 
  &lt;property&gt;
    &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;
    &lt;value&gt;/usr/local/hive/hive-2.3.4/tmp/hadoop/operation_logs&lt;/value&gt;
    &lt;description&gt;Top level directory where operation logs are stored if logging functionality is enabled&lt;/description&gt;
  &lt;/property&gt;
  
  &lt;property&gt;
    &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;
    &lt;value&gt;/usr/local/hive/hive-2.3.4/tmp/hadoop&lt;/value&gt;
    &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;
  &lt;/property&gt;
  
  &lt;property&gt;
    &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;
    &lt;value&gt;/usr/local/hive/hive-2.3.4/tmp/${hive.session.id}_resources&lt;/value&gt;
    &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;
  &lt;/property&gt;
  
  &lt;property&gt;
    &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
    &lt;description&gt;
      Enforce metastore schema version consistency.
      True: Verify that version information stored in is compatible with one from Hive jars.  Also disable automatic
            schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures
            proper metastore schema migration. (Default)
      False: Warn if the version information stored in metastore doesn't match with one from in Hive jars.
    &lt;/description&gt;
  &lt;/property&gt;

</code></pre>
<h3 id="配置hive-envsh文件">配置hive-env.sh文件</h3>
<p>复制一份配置文件</p>
<pre><code>cp hive-env.sh.template hive-env.sh

</code></pre>
<p>找到如下位置，做对应修改</p>
<pre><code># Set HADOOP_HOME to point to a specific hadoop install directory
HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.7

# Hive Configuration Directory can be controlled by:
export HIVE_CONF_DIR=/usr/local/hive/hive-2.3.4/conf

# Folder containing extra libraries required for hive compilation/execution can be controlled by:
# export HIVE_AUX_JARS_PATH=
export JAVA_HOME=/usr/local/java/jdk1.8.0_201
export HIVE_HOME=/usr/local/hive/hive-2.3.4
</code></pre>
<p>如图所示：</p>
<figure data-type="image" tabindex="33"><img src="https://i.loli.net/2019/04/09/5cac16eb109b4.png" alt="" loading="lazy"></figure>
<h3 id="下载mysql-connector-javajar">下载mysql-connector-java.jar</h3>
<p>将该jar包放进<code>/usr/local/hive/hive-2.3.4/lib/</code>中即可</p>
<h3 id="安装并配置mysql">安装并配置mysql</h3>
<p>安装mysql-server</p>
<figure data-type="image" tabindex="34"><img src="https://i.loli.net/2019/04/09/5cac1704194cb.png" alt="" loading="lazy"></figure>
<p>完成之后，<code>systemctl status mysql.service</code>可以查看当前状态</p>
<figure data-type="image" tabindex="35"><img src="https://i.loli.net/2019/04/09/5cac172ee68ce.png" alt="" loading="lazy"></figure>
<h3 id="mysql上创建hive元数据库创建hive账号并进行授权">Mysql上创建hive元数据库，创建hive账号，并进行授权</h3>
<h2 id="安装zookeeper">安装Zookeeper</h2>
<h3 id="介绍-3">介绍</h3>
<blockquote>
<p>ZooKeeper是一个分布式的应用程序协调服务，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。其目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。</p>
<p>ZooKeeper是一个分布式的应用程序协调服务，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。其目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。</p>
<p>那么Zookeeper能做什么事情呢？举个简单的例子：假设我们有20个搜索引擎的服务器(每个负责总索引中的一部分的搜索任务)和一个总服务器(负责向这20个搜索引擎的服务器发出搜索请求并合并结果集)，一个备用的总服务器(负责当总服务器宕机时替换总服务器)，一个web的cgi(向总服务器发出搜索请求)。搜索引擎的服务器中的15个服务器提供搜索服务，5个服务器正在生成索引。这20个搜索引擎的服务器经常要让正在提供搜索服务的服务器停止提供服务开始生成索引，或生成索引的服务器已经把索引生成完成可以提供搜索服务了。使用Zookeeper可以保证总服务器自动感知有多少提供搜索引擎的服务器并向这些服务器发出搜索请求，当总服务器宕机时自动启用备用的总服务器。</p>
</blockquote>
<h3 id="环境配置-2">环境配置</h3>
<p>使用winscp上传zookeeper安装包</p>
<p>新建文件夹<code>mkdir zookeeper</code></p>
<p>解压缩<code>tar -zxvf zookeeper-3.4.10.tar.gz</code></p>
<figure data-type="image" tabindex="36"><img src="https://i.loli.net/2019/04/09/5cac2be852fc3.png" alt="" loading="lazy"></figure>
<p>编辑<code>vim /etc/profile</code>，对比添加和修改：</p>
<pre><code>export JAVA_HOME=/usr/local/java/jdk1.8.0_191
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.7
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HIVE_HOME=/usr/local/hive/hive-2.3.4
export ZOOKEEPER_HOME=/usr/local/zookeeper/zookeeper-3.4.10
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$ZOOKEEPER_HOME/bin

</code></pre>
<p>如图所示：</p>
<figure data-type="image" tabindex="37"><img src="https://i.loli.net/2019/04/09/5cac2c2ce9312.png" alt="" loading="lazy"></figure>
<p>保存退出并<code>source /etc/profile</code>使其生效</p>
<h3 id="配置zoocfg文件">配置zoo.cfg文件</h3>
<pre><code>cd /usr/local/zookeeper/zookeeper-3.4.10/conf
cp zoo_sample.cfg zoo.cfg

</code></pre>
<p>将datadir修改为自己的地址，并在文件末尾加上如图所示字段</p>
<figure data-type="image" tabindex="38"><img src="https://i.loli.net/2019/04/09/5cac2cf42fc03.png" alt="" loading="lazy"></figure>
<h3 id="配置myid文件">配置myid文件</h3>
<pre><code>cd ..
mkdir data 
cd data
vim myid

</code></pre>
<p>在myid文件中输入<code>0</code>即可</p>
<figure data-type="image" tabindex="39"><img src="https://i.loli.net/2019/04/09/5cac2d8bdf835.png" alt="" loading="lazy"></figure>
<h3 id="测试-2">测试</h3>
<p>在<code>usr/local/zookeeper/zookeeper-3.4.10</code>目录中</p>
<p>输入<code>bin/zkServer.sh start</code>即可启动zookeeper服务</p>
<p>jps查看，有QuromPeerMain进程表示启动成功。</p>
<figure data-type="image" tabindex="40"><img src="https://i.loli.net/2019/04/09/5cac2e3a5b712.png" alt="" loading="lazy"></figure>
<h2 id="安装kafka">安装Kafka</h2>
<h3 id="介绍-4">介绍</h3>
<blockquote>
<p>Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。Producer即生产者，向Kafka集群发送消息，在发送消息之前，会对消息进行分类，即主题（Topic），通过对消息指定主题可以将消息分类，消费者可以只关注自己需要的Topic中的消息。Consumer，即消费者，消费者通过与kafka集群建立长连接的方式，不断地从集群中拉取消息，然后可以对这些消息进行处理。</p>
</blockquote>
<h3 id="安装scala">安装Scala</h3>
<p>Kafka由Java和Scala编写，所以我们先要安装配置Scala</p>
<pre><code>cd /usr/local
mkdir scala
cd scala/
#用winscp把scala安装包上传到该文件夹并解压
tar -zxvf scala-2.11.8.tgz

</code></pre>
<figure data-type="image" tabindex="41"><img src="https://i.loli.net/2019/04/09/5cac2f658ca90.png" alt="" loading="lazy"></figure>
<h3 id="配置环境变量-2">配置环境变量</h3>
<p>进入<code>vim /etc/profile</code>，对比修改：</p>
<pre><code>export JAVA_HOME=/usr/local/java/jdk1.8.0_191
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.7
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HIVE_HOME=/usr/local/hive/hive-2.3.4
export ZOOKEEPER_HOME=/usr/local/zookeeper/zookeeper-3.4.10
export SCALA_HOME=/usr/local/scala/scala-2.11.8
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$ZOOKEEPER_HOME/bin:$SCALA_HOME/bin

</code></pre>
<p>如图所示：</p>
<figure data-type="image" tabindex="42"><img src="https://i.loli.net/2019/04/09/5cac2f847e2cc.png" alt="" loading="lazy"></figure>
<p>保存退出并<code>source /etc/profile</code>使其生效</p>
<p>验证是否安装成功，输入<code>scala -version</code>即可查看版本信息</p>
<figure data-type="image" tabindex="43"><img src="https://i.loli.net/2019/04/09/5cac2f9c8b962.png" alt="" loading="lazy"></figure>
<h3 id="配置kafka">配置Kafka</h3>
<p>创建目录，把压缩包用winscp上传</p>
<pre><code>mkdir kafka
cd kafka
tar -zxvf kafka_2.11-2.1.0.tgz
mv kafka_2.11-2.1.0 kafka-2.1.0

</code></pre>
<figure data-type="image" tabindex="44"><img src="https://i.loli.net/2019/04/09/5cac2fc48d4f5.png" alt="" loading="lazy"></figure>
<h3 id="修改serverproperties文件">修改server.properties文件</h3>
<p><code>vim kafka-2.1.0/config/server.properties</code></p>
<p>找到如下字段并对应修改</p>
<pre><code>broker.id=0
listeners=PLAINTEXT://192.168.6.22:9092
advertised.listeners=PLAINTEXT://192.168.6.22:9092
zookeeper.connect=192.168.6.22:2181

</code></pre>
<p>如图</p>
<figure data-type="image" tabindex="45"><img src="https://i.loli.net/2019/04/09/5cac2fe843ebb.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="46"><img src="https://i.loli.net/2019/04/09/5cac3006eb331.png" alt="" loading="lazy"></figure>
<h3 id="测试-3">测试</h3>
<p>启动kafka</p>
<pre><code>cd kafka/kafka-2.1.0/
bin/kafka-server-start.sh config/server.properties &amp;

</code></pre>
<p>jps查看进程</p>
<figure data-type="image" tabindex="47"><img src="https://i.loli.net/2019/04/09/5cac301f3c759.png" alt="" loading="lazy"></figure>
<h2 id="安装flume">安装Flume</h2>
<h3 id="介绍-5">介绍</h3>
<blockquote>
<p>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。Flume提供了从console（控制台）、RPC（Thrift-RPC）、text（文件）、tail（UNIX tail）、syslog（syslog日志系统），支持TCP和UDP等2种模式），exec（命令执行）等数据源上收集数据的能力。</p>
<p>使用Flume，我们可以将从多个服务器中获取的数据迅速的移交给Hadoop中，可以高效率的将多个网站服务器中收集的日志信息存入HDFS/HBase中。</p>
</blockquote>
<h3 id="环境配置-3">环境配置</h3>
<p>创建目录，使用winscp把压缩包上传</p>
<pre><code>mkdir flume
cd flume
tar -zxvf apache-flume-1.8.0-bin.tar.gz 
mv apache-flume-1.8.0-bin flume-1.8.0

</code></pre>
<p>如图：</p>
<figure data-type="image" tabindex="48"><img src="https://i.loli.net/2019/04/09/5cac30389c66e.png" alt="" loading="lazy"></figure>
<p>打开<code>vim /etc/profile</code>，如下修改</p>
<pre><code>export JAVA_HOME=/usr/local/java/jdk1.8.0_210
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.7
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HIVE_HOME=/usr/local/hive/hive-2.3.4
export ZOOKEEPER_HOME=/usr/local/zookeeper/zookeeper-3.4.10
export SCALA_HOME=/usr/local/scala/scala-2.11.8
export FLUME_HOME=/usr/local/flume/flume-1.8.0
export FLUME_CONF_DIR=$FLUME_HOME/conf
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$ZOOKEEPER_HOME/bin:$SCALA_HOME/bin:$FLUME_HOME/bin

</code></pre>
<p>如图所示：</p>
<figure data-type="image" tabindex="49"><img src="https://i.loli.net/2019/04/09/5cac305539db5.png" alt="" loading="lazy"></figure>
<p><code>source /etc/profile</code>使其生效</p>
<h3 id="修改flume-confproperties">修改flume-conf.properties</h3>
<pre><code>cd flume-1.8.0/conf
cp flume-conf.properties.template flume-conf.properties

</code></pre>
<p><code>vim flume-conf.properties</code>在文件最后加上如下内容：</p>
<pre><code>#agent1表示代理名称
agent1.sources=source1
agent1.sinks=sink1
agent1.channels=channel1
#配置source1
agent1.sources.source1.type=spooldir
agent1.sources.source1.spoolDir=/usr/local/flume/logs
agent1.sources.source1.channels=channel1
agent1.sources.source1.fileHeader = false
agent1.sources.source1.interceptors = i1
agent1.sources.source1.interceptors.i1.type = timestamp
#配置channel1
agent1.channels.channel1.type=file
agent1.channels.channel1.checkpointDir=/usr/local/flume/logs_tmp_cp
agent1.channels.channel1.dataDirs=/usr/local/flume/logs_tmp
#配置sink1
agent1.sinks.sink1.type=hdfs
agent1.sinks.sink1.hdfs.path=hdfs://master:9000/logs
agent1.sinks.sink1.hdfs.fileType=DataStream
agent1.sinks.sink1.hdfs.writeFormat=TEXT
agent1.sinks.sink1.hdfs.rollInterval=1
agent1.sinks.sink1.channel=channel1
agent1.sinks.sink1.hdfs.filePrefix=%Y-%m-%d

</code></pre>
<p>因为上面监听的文件夹是<code>usr/local/flume/logs</code>所以我们要手动创建</p>
<pre><code>cd /usr/local/flume
mkdir logs

</code></pre>
<figure data-type="image" tabindex="50"><img src="https://i.loli.net/2019/04/09/5cac306e31d14.png" alt="" loading="lazy"></figure>
<p>上面的配置文件中 <code>agent1.sinks.sink1.hdfs.path=hdfs://master:9000/logs</code>下，即将监听到的<code>/usr/local/flume/logs</code>下的文件自动上传到hdfs的<code>/logs</code>下，所以我们要手动创建hdfs下的目录</p>
<pre><code>hdfs dfs -mkdir /logs 
</code></pre>
<p>未完待续….</p>
<h2 id="安装hbase">安装Hbase</h2>
<h3 id="介绍-6">介绍</h3>
<blockquote>
<p>HBase – Hadoop Database，是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统。Hadoop HDFS为HBase提供了高可靠性的底层存储支持，Hadoop MapReduce为HBase提供了高性能的计算能力，Zookeeper为HBase提供了稳定服务和failover机制。</p>
</blockquote>
<h3 id="环境配置-4">环境配置</h3>
<p>创建目录，使用winscp将压缩包上传</p>
<pre><code>mkdir hbase
cd hbase
tar -zxvf hbase-2.1.1-bin.tar.gz
</code></pre>
<figure data-type="image" tabindex="51"><img src="https://i.loli.net/2019/04/09/5cac310163575.png" alt="" loading="lazy"></figure>
<p>打开<code>vim /etc/profile</code>环境变量</p>
<pre><code>export JAVA_HOME=/usr/local/java/jdk1.8.0_210
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.7
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HIVE_HOME=/usr/local/hive/hive-2.3.4
export ZOOKEEPER_HOME=/usr/local/zookeeper/zookeeper-3.4.10
export SCALA_HOME=/usr/local/scala/scala-2.11.8
export FLUME_HOME=/usr/local/flume/flume-1.8.0
export FLUME_CONF_DIR=$FLUME_HOME/conf
export HBASE_HOME=/usr/local/hbase/hbase-2.1.1
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$ZOOKEEPER_HOME/bin:$SCALA_HOME/bin:$FLUME_HOME/bin:$HBASE_HOME/bin

</code></pre>
<p>如图所示</p>
<figure data-type="image" tabindex="52"><img src="https://i.loli.net/2019/04/09/5cac3193bdd8a.png" alt="" loading="lazy"></figure>
<p>保存退出并<code>source /etc/profile</code>使其生效</p>
<h3 id="配置hbase-envsh">配置hbase-env.sh</h3>
<pre><code>cd hbase-2.1.1/conf
vim hbase-env.sh

#后面添加如下字段
export JAVA_HOME=/usr/local/java/jdk1.8.0_210
export HBASE_LOG_DIR=${HBASE_HOME}/logs 
export HBASE_MANAGES_ZK=false

</code></pre>
<p>如图</p>
<figure data-type="image" tabindex="53"><img src="https://i.loli.net/2019/04/09/5cac31b765090.png" alt="" loading="lazy"></figure>
<h3 id="配置hbase-sitexml">配置hbase-site.xml</h3>
<p>编辑<code>vim hbase-site.xml</code></p>
<pre><code>&lt;configuration&gt;
&lt;property&gt; 
    &lt;name&gt;hbase.rootdir&lt;/name&gt; 
    &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; 
  &lt;/property&gt; 
  &lt;property&gt; 
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; 
    &lt;value&gt;true&lt;/value&gt; 
  &lt;/property&gt; 
  &lt;property&gt; 
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; 
    &lt;value&gt;master,slave1,slave2&lt;/value&gt; 
  &lt;/property&gt; 
  &lt;property&gt; 
    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; 
    &lt;value&gt;/usr/local/zookeeper/zookeeper-3.4.10/data&lt;/value&gt; 
  &lt;/property&gt; 
  &lt;property&gt; 
    &lt;name&gt;hbase.master&lt;/name&gt; 
    &lt;value&gt;hdfs://master:60000&lt;/value&gt; 
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.master.info.port&lt;/name&gt;
    &lt;value&gt;16010&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.regionserver.info.port&lt;/name&gt;
    &lt;value&gt;16030&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;

</code></pre>
<figure data-type="image" tabindex="54"><img src="https://i.loli.net/2019/04/09/5cac31d484fb2.png" alt="" loading="lazy"></figure>
<h3 id="配置regionservers文件">配置regionservers文件</h3>
<p>因为我这里只有一个master节点，故文件里只修改master</p>
<pre><code>vim regionservers 
master

</code></pre>
<figure data-type="image" tabindex="55"><img src="https://i.loli.net/2019/04/09/5cac31f5d19b3.png" alt="" loading="lazy"></figure>
<h3 id="测试-4">测试</h3>
<pre><code>cd hbase/hbase-2.1.1
bin/start-hbase.sh   
jps

</code></pre>
<p>查看是否启动了<strong>HMaster</strong>和<strong>HRegionServer</strong>，如图所示</p>
<figure data-type="image" tabindex="56"><img src="https://i.loli.net/2019/04/09/5cac320ebc60f.png" alt="" loading="lazy"></figure>
<h2 id="安装spark">安装Spark</h2>
<h3 id="介绍-7">介绍</h3>
<blockquote>
<p>Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎，是类似于Hadoop MapReduce的通用并行框架。Spark拥有Hadoop MapReduce所具有的优点，但不同于MapReduce的是——Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。Spark实际上是对Hadoop的一种补充，可以很好的在Hadoop 文件系统中并行运行。</p>
</blockquote>
<h3 id="环境配置-5">环境配置</h3>
<p>创建目录，使用winscp将压缩包上传</p>
<pre><code>mkdir spark
cd spark
tar -zxvf spark-2.4.0-bin-hadoop2.7.tgz 
mv spark-2.4.0-bin-hadoop2.7 spark-2.4.0

</code></pre>
<figure data-type="image" tabindex="57"><img src="https://i.loli.net/2019/04/09/5cac32255d68f.png" alt="" loading="lazy"></figure>
<p>修改系统变量</p>
<pre><code>vim /etc/profile
export JAVA_HOME=/usr/local/jdk/jdk1.8.0_201
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.7
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HIVE_HOME=/usr/local/hive/hive-2.3.4
export ZOOKEEPER_HOME=/usr/local/zookeeper/zookeeper-3.4.10
export SCALA_HOME=/usr/local/scala/scala-2.11.8
export FLUME_HOME=/usr/local/flume/flume-1.8.0
export FLUME_CONF_DIR=$FLUME_HOME/conf
export HBASE_HOME=/usr/local/hbase/hbase-2.1.1
export SPARK_HOME=/usr/local/spark/spark-2.4.0
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$ZOOKEEPER_HOME/bin:$SCALA_HOME/bin:$FLUME_HOME/bin:$HBASE_HOME/bin:$SPARK_HOME/bin

</code></pre>
<p>保存退出并<code>source /etc/profile</code></p>
<figure data-type="image" tabindex="58"><img src="https://i.loli.net/2019/04/09/5cac323a76b22.png" alt="" loading="lazy"></figure>
<h3 id="配置spark-envsh文件">配置spark-env.sh文件</h3>
<pre><code>cd spark-2.4.0/conf/
cp spark-env.sh.template spark-env.sh
vim spark-env.sh 
#
export JAVA_HOME=/usr/local/jdk/jdk1.8.0_201
export SCALA_HOME=/usr/local/scala/scala-2.11.8
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.7
export HADOOP_CONF_DIR=/usr/local/hadoop/hadoop-2.7.7/etc/hadoop
</code></pre>
<h3 id="配置slaves文件">配置slaves文件</h3>
<pre><code>mv slaves.template slaves
vim slaves
master
</code></pre>
<p>如图所示：</p>
<figure data-type="image" tabindex="59"><img src="https://i.loli.net/2019/04/09/5cac3266773cb.png" alt="" loading="lazy"></figure>
<p>打开浏览器<strong>http://192.168.6.22:8080/</strong></p>
<figure data-type="image" tabindex="60"><img src="https://i.loli.net/2019/04/09/5cac32a8d2fb6.png" alt="" loading="lazy"></figure>
<h3 id="测试-5">测试</h3>
<p>使用Spark自带的计算圆周率做测试</p>
<pre><code>./bin/spark-submit  \
--class  org.apache.spark.examples.SparkPi  \
--master  local  \
examples/jars/spark-examples_2.11-2.4.0.jar
</code></pre>
<p>在控制台中可以找到输出的结果</p>
<figure data-type="image" tabindex="61"><img src="https://i.loli.net/2019/04/09/5cac32c7b9e27.png" alt="" loading="lazy"></figure>

        </div>
        <!-- Share to Twitter, Weibo, Telegram -->
        <div class="flex items-center">
          <div class="mr-4 flex items-center">
            <i class="ri-share-forward-line text-gray-500"></i>
          </div>
          <div class="px-4 cursor-pointer text-blue-500 hover:bg-blue-100 dark:hover:bg-gray-600 inline-flex" @click="shareToTwitter">
            <i class="ri-twitter-line"></i>
          </div>
          <div class="px-4 cursor-pointer text-red-500 hover:bg-red-100 dark:hover:bg-gray-600 inline-flex" @click="shareToWeibo">
            <i class="ri-weibo-line"></i>
          </div>
          <div class="px-4 cursor-pointer text-indigo-500 hover:bg-indigo-100 dark:hover:bg-gray-600 inline-flex" @click="shareToTelegram">
            <i class="ri-telegram-line"></i>
          </div>
        </div>
      </div>

      
        
          <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script type="application/javascript" src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script type="application/javascript">

  var gitalk = new Gitalk({
    clientID: '0102793645acc8ba1446',
    clientSecret: 'dbc9e05fb6ac3866c98a32d4af38e4494934965c',
    repo: 'tediorelee.github.io',
    owner: 'tediorelee',
    admin: ['tediorelee'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

        

        
      

      

      <footer class="py-12 text-center px-4 md:px-0" v-pre>
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> Published by <a href="https://iidx.xyz">TEDIORELEE</a>
</footer>
    </div>

    <!-- TOC Container -->
    <div class="fixed right-0 bottom-0 mb-16 mr-4 shadow w-8 h-8 rounded-full flex justify-center items-center z-10 cursor-pointer bg-white dark:bg-gray-500 dark:text-gray-200 hover:shadow-lg transition-all animated fadeInRight" @click="showToc = true">
      <i class="ri-file-list-line"></i>
    </div>

    <div class="fixed right-0 top-0 bottom-0 overflow-y-auto w-64 bg-white dark:bg-gray-800 p-4 border-l border-gray-100 dark:border-gray-600 z-10 transition-fast" :class="{ '-mr-64': !showToc }">
      <div class="flex mb-4 justify-end">
        <div class="w-8 h-8 inline-flex justify-center items-center rounded-full cursor-pointer hover:bg-gray-200 dark:hover:bg-gray-600 transition-fast" @click="showToc = false">
          <i class="ri-close-line text-lg"></i>
        </div>
      </div>
      <div class="post-toc-container">
        <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#%E5%AE%89%E8%A3%85%E8%99%9A%E6%8B%9F%E6%9C%BA">安装虚拟机</a>
<ul>
<li><a href="#%E4%BF%AE%E6%94%B9ubuntu%E7%9A%84root%E7%94%A8%E6%88%B7%E5%90%8D">修改Ubuntu的root用户名</a></li>
<li><a href="#%E9%85%8D%E7%BD%AE%E9%9D%99%E6%80%81ip">配置静态IP</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEssh%E7%99%BB%E5%BD%95">配置SSH登录</a></li>
<li><a href="#%E4%BF%AE%E6%94%B9%E8%BD%AF%E4%BB%B6%E6%BA%90">修改软件源</a></li>
</ul>
</li>
<li><a href="#%E5%AE%89%E8%A3%85jdk">安装JDK</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEssh%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95">配置SSH免密登录</a></li>
<li><a href="#%E5%AE%89%E8%A3%85hadoop">安装Hadoop</a>
<ul>
<li><a href="#%E4%BB%8B%E7%BB%8D">介绍</a></li>
<li><a href="#%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F">配置环境变量</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEcore-sitexml">配置core-site.xml</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEhdfs-sitexml">配置hdfs-site.xml</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEmapred-sitexml">配置mapred-site.xml</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEyarn-sitexml">配置yarn-site.xml</a></li>
<li><a href="#%E6%B5%8B%E8%AF%95">测试</a></li>
</ul>
</li>
<li><a href="#%E5%AE%89%E8%A3%85hive">安装Hive</a>
<ul>
<li><a href="#%E4%BB%8B%E7%BB%8D-2">介绍</a></li>
<li><a href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE">环境配置</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEhive-sitexml">配置hive-site.xml</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEhive-envsh%E6%96%87%E4%BB%B6">配置hive-env.sh文件</a></li>
<li><a href="#%E4%B8%8B%E8%BD%BDmysql-connector-javajar">下载mysql-connector-java.jar</a></li>
<li><a href="#%E5%AE%89%E8%A3%85%E5%B9%B6%E9%85%8D%E7%BD%AEmysql">安装并配置mysql</a></li>
<li><a href="#mysql%E4%B8%8A%E5%88%9B%E5%BB%BAhive%E5%85%83%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%9B%E5%BB%BAhive%E8%B4%A6%E5%8F%B7%E5%B9%B6%E8%BF%9B%E8%A1%8C%E6%8E%88%E6%9D%83">Mysql上创建hive元数据库，创建hive账号，并进行授权</a></li>
</ul>
</li>
<li><a href="#%E5%AE%89%E8%A3%85zookeeper">安装Zookeeper</a>
<ul>
<li><a href="#%E4%BB%8B%E7%BB%8D-3">介绍</a></li>
<li><a href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-2">环境配置</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEzoocfg%E6%96%87%E4%BB%B6">配置zoo.cfg文件</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEmyid%E6%96%87%E4%BB%B6">配置myid文件</a></li>
<li><a href="#%E6%B5%8B%E8%AF%95-2">测试</a></li>
</ul>
</li>
<li><a href="#%E5%AE%89%E8%A3%85kafka">安装Kafka</a>
<ul>
<li><a href="#%E4%BB%8B%E7%BB%8D-4">介绍</a></li>
<li><a href="#%E5%AE%89%E8%A3%85scala">安装Scala</a></li>
<li><a href="#%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F-2">配置环境变量</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEkafka">配置Kafka</a></li>
<li><a href="#%E4%BF%AE%E6%94%B9serverproperties%E6%96%87%E4%BB%B6">修改server.properties文件</a></li>
<li><a href="#%E6%B5%8B%E8%AF%95-3">测试</a></li>
</ul>
</li>
<li><a href="#%E5%AE%89%E8%A3%85flume">安装Flume</a>
<ul>
<li><a href="#%E4%BB%8B%E7%BB%8D-5">介绍</a></li>
<li><a href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-3">环境配置</a></li>
<li><a href="#%E4%BF%AE%E6%94%B9flume-confproperties">修改flume-conf.properties</a></li>
</ul>
</li>
<li><a href="#%E5%AE%89%E8%A3%85hbase">安装Hbase</a>
<ul>
<li><a href="#%E4%BB%8B%E7%BB%8D-6">介绍</a></li>
<li><a href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-4">环境配置</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEhbase-envsh">配置hbase-env.sh</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEhbase-sitexml">配置hbase-site.xml</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEregionservers%E6%96%87%E4%BB%B6">配置regionservers文件</a></li>
<li><a href="#%E6%B5%8B%E8%AF%95-4">测试</a></li>
</ul>
</li>
<li><a href="#%E5%AE%89%E8%A3%85spark">安装Spark</a>
<ul>
<li><a href="#%E4%BB%8B%E7%BB%8D-7">介绍</a></li>
<li><a href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-5">环境配置</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEspark-envsh%E6%96%87%E4%BB%B6">配置spark-env.sh文件</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEslaves%E6%96%87%E4%BB%B6">配置slaves文件</a></li>
<li><a href="#%E6%B5%8B%E8%AF%95-5">测试</a></li>
</ul>
</li>
</ul>
</li>
</ul>

      </div>
    </div>

    <!-- Back to top -->
    <div class="fixed right-0 bottom-0 mb-4 mr-4 shadow w-8 h-8 rounded-full flex justify-center items-center z-10 cursor-pointer bg-white hover:shadow-lg transition-all dark:bg-gray-500 dark:text-gray-200" @click="backToUp" v-show="scrolled">
      <i class="ri-arrow-up-line"></i>
    </div>
  </div>

  <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
  <!-- Background of PhotoSwipe. 
        It's a separate element as animating opacity is faster than rgba(). -->
  <div class="pswp__bg">
  </div>
  <!-- Slides wrapper with overflow:hidden. -->
  <div class="pswp__scroll-wrap">
    <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
    <div class="pswp__container">
      <div class="pswp__item">
      </div>
      <div class="pswp__item">
      </div>
      <div class="pswp__item">
      </div>
    </div>
    <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
    <div class="pswp__ui pswp__ui--hidden">
      <div class="pswp__top-bar">
        <!--  Controls are self-explanatory. Order can be changed. -->
        <div class="pswp__counter">
        </div>
        <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
        <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
        <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
        <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
        <!-- element will get class pswp__preloader--active when preloader is running -->
        <div class="pswp__preloader">
          <div class="pswp__preloader__icn">
            <div class="pswp__preloader__cut">
              <div class="pswp__preloader__donut">
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
        <div class="pswp__share-tooltip">
        </div>
      </div>
      <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
      </button>
      <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
      </button>
      <div class="pswp__caption">
        <div class="pswp__caption__center">
        </div>
      </div>
    </div>
  </div>
</div>

  <script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>
  <script src="https://leejieun.fan/media/scripts/main.js"></script>
  
  <!-- Code Highlight -->
  
    <script src="https://leejieun.fan/media/prism.js"></script>
    <script>
      Prism.highlightAll()
    </script>
  

  <script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>
  <script>
    //拿到预览框架，也就是上面的html代码
    var pswpElement = document.querySelectorAll('.pswp')[0];
    //定义图片数组变量
    var imgitems;
    /**
    * 用于显示预览界面
    * @param index 图片数组下标
    */
    function viewImg(index) {
      //其它选项这里不做过多阐述，详情见官网
      var pswpoptions = {
        index: parseInt(index, 10), // 开始幻灯片索引。0是第一张幻灯片。必须是整数，而不是字符串。
        bgOpacity: 0.7, // 背景透明度，0-1
        maxSpreadZoom: 3, // 缩放级别，不要太大
      };
      //初始化并打开PhotoSwipe，pswpElement对应上面预览框架，PhotoSwipeUI_Default为皮肤，imgitems为图片数组，pswpoptions为选项
      var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, imgitems, pswpoptions);
      gallery.init()
    }
    /**
    * 用于添加图片点击事件
    * @param img 图片元素
    * @param index 所属下标（在imgitems中的位置）
    */
    function addImgClick(img, index) {
      img.onclick = function() {
        viewImg(index)
      }
    }
    /**
    * 轮询所有图片，获取src、width、height等数据，加入imgitems，并给图片元素添加事件
    * 最好在onload中执行该方法，本站因放在最底部，所以直接初始化
    * 异步加载图片可在图片元素创建完成后调用此方法
    */
    function initImg() {
      //重置图片数组
      imgitems = [];
      //查找class:markdown 下的所有img元素并遍历
      var imgs = document.querySelectorAll('.markdown img');
      for (var i = 0; i < imgs.length; i++) {
        var img = imgs[i];
        //本站相册初始为loading图片，真实图片放在data-src
        var ds = img.getAttribute("data-src");
        //创建image对象，用于获取图片宽高
        var imgtemp = new Image();
        //判断是否存在data-src
        if (ds != null && ds.length > 0) {
          imgtemp.src = ds
        } else {
          imgtemp.src = img.src
        }
        //判断是否存在缓存
        if (imgtemp.complete) {
          var imgobj = {
            "src": imgtemp.src,
            "w": imgtemp.width,
            "h": imgtemp.height,
          };
          imgitems[i] = imgobj;
          addImgClick(img, i);
        } else {
          console.log('进来了2')
          imgtemp.index = i;
          imgtemp.img = img;
          imgtemp.onload = function() {
            var imgobj = {
              "src": this.src,
              "w": this.width,
              "h": this.height,
            };
            //不要使用push，因为onload前后顺序会不同
            imgitems[this.index] = imgobj
            //添加点击事件
            addImgClick(this.img, this.index);
          }
        }
      }
    }
    //初始化
    initImg();
  </script>
</body>

</html>